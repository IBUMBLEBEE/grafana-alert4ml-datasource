use tonic::{transport::Server, Request, Response, Status};
use arrow::ipc::reader::{StreamReader, FileReader};
use arrow::array::{Float64Array, RecordBatch, Int64Array, Array as ArrowArray};
use arrow::datatypes::{DataType, Field, Schema};
use arrow::ipc::writer::StreamWriter;
use polars::prelude::*;
use polars::prelude::DataType as PolarsDataType;
use std::io::Cursor;
use std::sync::Arc;
use std::path::Path;
use tokio::net::UnixListener;
use tokio::signal;
use tokio_stream::wrappers::UnixListenerStream;
use rsod_storage::init_db;
use rsod_outlier::outlier;
use rsod_baseline::{baseline_detect, BaselineOptions};
use rsod_forecaster::{forecast, ForecasterOptions};

mod rsod;
use rsod as rsodsvc;

use rsodsvc::rsod_service_server::{RsodService, RsodServiceServer};
use rsodsvc::{HealthRequest, HealthResponse, DetectOutliersRequest, DetectOutliersResponse, DetectBaselineRequest, DetectBaselineResponse, ForecastRequest, ForecastResponse};

#[derive(Debug, Default)]
pub struct RsodServiceImpl {}

#[tonic::async_trait]
impl RsodService for RsodServiceImpl {
    async fn health(&self, _request: Request<HealthRequest>) -> Result<Response<HealthResponse>, Status> {
        Ok(Response::new(HealthResponse { healthy: true, version: "0.1.0".to_string() }))
    }

    async fn detect_outliers(&self, request: Request<DetectOutliersRequest>) -> Result<Response<DetectOutliersResponse>, Status> {
        let req = request.into_inner();
        let data_vec = match convert_to_points(req.data_frame) {
            Ok(data) => data,
            Err(_) => {
                return Err(Status::invalid_argument("Invalid input data format"));
            }
        };
        let opts = req.options.ok_or(Status::invalid_argument("Missing outlier options"))?;
        let periods = opts.periods.iter().map(|x| *x as usize).collect::<Vec<usize>>();
        let outlier_result = match outlier(&data_vec, &periods, &opts.uuid) {
            Ok(result) => result,
            Err(_) => {
                return Err(Status::internal("Failed to detect outliers"));
            }
        };
        let result_data = match convert_to_arrow_ipc(outlier_result) {
            Ok(data) => data,
            Err(_) => {
                return Err(Status::internal("Failed to convert result to Arrow IPC format"));
            }
        };
        Ok(Response::new(DetectOutliersResponse { result_data, error_message: "".to_string() }))
    }

    async fn detect_baseline(&self, request: Request<DetectBaselineRequest>) -> Result<Response<DetectBaselineResponse>, Status> {
        let req = request.into_inner();
        let current_data_vec = match convert_to_points(req.current_data) {
            Ok(data) => data,
            Err(_) => {
                return Err(Status::invalid_argument("Invalid input data format"));
            }
        };
        let history_data_vec = match convert_to_points(req.history_data) {
            Ok(data) => data,
            Err(_) => {
                return Err(Status::invalid_argument("Invalid input data format"));
            }
        };
        let optssvc: rsodsvc::BaselineOptions = req.options.ok_or(Status::invalid_argument("Missing baseline options"))?;
        let opts = BaselineOptions {
            uuid: optssvc.uuid,
            trend_type: match optssvc.trend_type {
                1 => rsod_baseline::TrendType::Daily,
                2 => rsod_baseline::TrendType::Weekly,
                3 => rsod_baseline::TrendType::Monthly,
                _ => rsod_baseline::TrendType::Daily, // default
            },
            interval_minutes: Some(optssvc.interval_mins as u32),
            confidence_level: Some(optssvc.confidence_level),
            allow_negative_bounds: Some(optssvc.allow_negative_bounds),
            std_dev_multiplier: Some(optssvc.std_dev_multiplier),
        };
        let baseline_result = match baseline_detect(&current_data_vec, &history_data_vec, &opts) {
            Ok(result) => result,
            Err(_) => {
                return Err(Status::internal("Failed to detect baseline"));
            }
        };
        let result_data = match convert_record_batch_to_arrow_ipc(baseline_result) {
            Ok(data) => data,
            Err(_) => {
                return Err(Status::internal("Failed to convert baseline result to Arrow IPC format"));
            }
        };
        Ok(Response::new(DetectBaselineResponse { result_frame: result_data, error_message: "".to_string() }))
    }

    async fn forecast(&self, request: Request<ForecastRequest>) -> Result<Response<ForecastResponse>, Status> {
        let req = request.into_inner();

        // æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
        println!("ğŸ“Š Received data sizes - current_data: {} bytes, history_data: {} bytes", 
                 req.current_data.len(), req.history_data.len());

        // Convert current data
        let current_data_vec = match convert_to_points(req.current_data) {
            Ok(data) => {
                println!("âœ… Successfully converted current data: {} points", data.len());
                data
            },
            Err(e) => {
                eprintln!("âŒ Failed to convert current data: {}", e);
                return Err(Status::invalid_argument(format!("Invalid current data format: {}", e)));
            }
        };

        // Convert history data
        let history_data_vec = match convert_to_points(req.history_data) {
            Ok(data) => {
                println!("âœ… Successfully converted history data: {} points", data.len());
                data
            },
            Err(e) => {
                eprintln!("âŒ Failed to convert history data: {}", e);
                return Err(Status::invalid_argument(format!("Invalid history data format: {}", e)));
            }
        };

        let optssvc: rsodsvc::ForecasterOptions = req.options.ok_or(Status::invalid_argument("Missing forecaster options"))?;
        let opts = ForecasterOptions {
            model_name: optssvc.model_name,
            periods: optssvc.periods.iter().map(|x| *x as usize).collect(),
            uuid: optssvc.uuid,
            budget: Some(optssvc.budget as f32),
            num_threads: Some(optssvc.num_threads as usize),
            n_lags: Some(optssvc.n_lags as usize),
            std_dev_multiplier: Some(optssvc.std_dev_multiplier),
            allow_negative_bounds: Some(optssvc.allow_negative_bounds),
        };

        println!("ğŸš€ Forecast starting, current data length: {}, history data length: {}, options: {:?}", current_data_vec.len(), history_data_vec.len(), opts);

        let forecast_result = match forecast(&current_data_vec, &history_data_vec, &opts) {
            Ok(result) => result,
            Err(_) => {
                return Err(Status::internal("Failed to perform forecast"));
            }
        };

        // Convert DataFrame to Arrow RecordBatch
        let result_data = match dataframe_to_recordbatch_forecast(forecast_result) {
            Ok(data) => data,
            Err(_) => {
                return Err(Status::internal("Failed to convert forecast result to Arrow RecordBatch"));
            }
        };

        // Convert RecordBatch to Arrow IPC bytes
        let ipc_data = match convert_record_batch_to_arrow_ipc(result_data) {
            Ok(data) => data,
            Err(_) => {
                return Err(Status::internal("Failed to convert forecast result to Arrow IPC format"));
            }
        };

        Ok(Response::new(ForecastResponse { result_frame: ipc_data, error_message: "".to_string() }))
    }
}

pub fn convert_to_points(data: Vec<u8>) -> Result<Vec<[f64; 2]>, Box<dyn std::error::Error>> {
    // æ£€æŸ¥æ•°æ®æ˜¯å¦ä¸ºç©º
    if data.is_empty() {
        return Err("Arrow IPC data is empty".into());
    }

    println!("ğŸ” Parsing Arrow IPC data: {} bytes", data.len());
    
    // æ£€æŸ¥ Arrow IPC é­”æ•°ï¼ˆå‰8å­—èŠ‚ï¼‰
    // Arrow IPC Stream æ ¼å¼: "ARROW1\0\0" (8 bytes)
    // Arrow IPC File æ ¼å¼: "ARROW1\0\0" (8 bytes) + footer
    if data.len() >= 8 {
        let magic = &data[0..8];
        let magic_str = String::from_utf8_lossy(magic);
        println!("  â†’ Magic bytes: {:?} (hex: {:02x?})", magic_str, magic);
        
        // Arrow IPC é­”æ•°åº”è¯¥æ˜¯ "ARROW1\0\0"
        if !magic.starts_with(b"ARROW") {
            println!("  âš ï¸  Warning: Data doesn't start with Arrow IPC magic bytes");
        }
    }
    
    // æ‰“å°å‰64å­—èŠ‚çš„åå…­è¿›åˆ¶ï¼Œç”¨äºè°ƒè¯•
    let preview_len = std::cmp::min(64, data.len());
    let hex_preview: String = data[..preview_len]
        .iter()
        .map(|b| format!("{:02x}", b))
        .collect::<Vec<_>>()
        .join(" ");
    println!("  â†’ First {} bytes (hex): {}", preview_len, hex_preview);

    let mut all_points = Vec::new();
    let mut batch_count = 0;

    // æ•°æ®æ ¼å¼åˆ†æï¼š
    // å‰4å­—èŠ‚: [ff, ff, ff, ff] = 0xFFFFFFFF (å¯èƒ½æ˜¯ continuation token æˆ–æ¶ˆæ¯å¤§å°)
    // æ¥ä¸‹æ¥4å­—èŠ‚: [c8, 01, 00, 00] = 456 (å¯èƒ½æ˜¯æ¶ˆæ¯å¤§å°)
    // 
    // å°è¯•å¤šç§è§£ææ–¹å¼ï¼š
    
    // æ–¹å¼1: å¦‚æœå‰4å­—èŠ‚æ˜¯ continuation tokenï¼Œè·³è¿‡å®ƒ
    if data.len() >= 8 && &data[0..4] == [0xff, 0xff, 0xff, 0xff] {
        println!("  â†’ Detected continuation token at start, skipping first 4 bytes...");
        let data_without_token = &data[4..];
        
        // æ„é€ æ ‡å‡†çš„ Stream æ ¼å¼
        let arrow_magic = b"ARROW1\0\0";
        let mut stream_data = Vec::with_capacity(8 + data_without_token.len() + 4);
        stream_data.extend_from_slice(arrow_magic);
        stream_data.extend_from_slice(data_without_token);
        stream_data.extend_from_slice(&[0xff, 0xff, 0xff, 0xff]);
        
        let cursor_skip_token = Cursor::new(&stream_data);
        match StreamReader::try_new(cursor_skip_token, None) {
            Ok(mut reader) => {
                println!("  â†’ Using StreamReader after skipping continuation token");
                for batch_result in reader {
                    let batch = batch_result
                        .map_err(|e| format!("Failed to read batch: {}", e))?;
                    
                    batch_count += 1;
                    println!("ğŸ“¦ Processing batch #{}: {} columns, {} rows", 
                             batch_count, batch.num_columns(), batch.num_rows());

                    if batch.num_columns() < 2 {
                        return Err(format!("Expected at least 2 columns, got {}", batch.num_columns()).into());
                    }

                    let col_x = batch.column(0)
                        .as_any()
                        .downcast_ref::<Float64Array>()
                        .ok_or_else(|| format!("Column 0 is not Float64Array, actual type: {:?}", batch.column(0).data_type()))?;
                        
                    let col_y = batch.column(1)
                        .as_any()
                        .downcast_ref::<Float64Array>()
                        .ok_or_else(|| format!("Column 1 is not Float64Array, actual type: {:?}", batch.column(1).data_type()))?;

                    if col_x.len() != col_y.len() {
                        return Err(format!("Column length mismatch: X={}, Y={}", col_x.len(), col_y.len()).into());
                    }

                    println!("  â†’ Extracting {} points from columns", col_x.len());

                    all_points.reserve(col_x.len());
                    for i in 0..col_x.len() {
                        let x = if col_x.is_null(i) {
                            return Err(format!("Null value found in X column at index {}", i).into());
                        } else {
                            col_x.value(i)
                        };
                        
                        let y = if col_y.is_null(i) {
                            return Err(format!("Null value found in Y column at index {}", i).into());
                        } else {
                            col_y.value(i)
                        };
                        
                        all_points.push([x, y]);
                    }
                }
                
                if batch_count > 0 {
                    println!("âœ… Successfully parsed {} batches, total points: {}", batch_count, all_points.len());
                    return Ok(all_points);
                }
            }
            Err(e) => {
                println!("  â†’ StreamReader after skipping token failed: {}", e);
            }
        }
    }
    
    // æ–¹å¼2: ç›´æ¥æ·»åŠ  Stream å¤´éƒ¨ï¼ˆä¸è·³è¿‡ä»»ä½•å­—èŠ‚ï¼‰
    println!("  â†’ Trying to add Stream header to raw message...");
    let arrow_magic = b"ARROW1\0\0";
    let mut stream_data = Vec::with_capacity(8 + data.len() + 4);
    stream_data.extend_from_slice(arrow_magic);
    stream_data.extend_from_slice(&data);
    // æ·»åŠ  continuation token (0xFFFFFFFF) è¡¨ç¤ºæµç»“æŸ
    stream_data.extend_from_slice(&[0xff, 0xff, 0xff, 0xff]);
    
    let cursor_with_header = Cursor::new(&stream_data);
    match StreamReader::try_new(cursor_with_header, None) {
        Ok(mut reader) => {
            println!("  â†’ Using StreamReader (IPC Stream format)");
            for batch_result in reader {
                let batch = batch_result
                    .map_err(|e| format!("Failed to read batch from StreamReader: {}", e))?;
                
                batch_count += 1;
                println!("ğŸ“¦ Processing batch #{}: {} columns, {} rows", 
                         batch_count, batch.num_columns(), batch.num_rows());

                // æ£€æŸ¥åˆ—æ•°
                if batch.num_columns() < 2 {
                    return Err(format!("Expected at least 2 columns, got {}", batch.num_columns()).into());
                }

                // æå– X å’Œ Y åˆ—
                let col_x = batch.column(0)
                    .as_any()
                    .downcast_ref::<Float64Array>()
                    .ok_or_else(|| format!("Column 0 is not Float64Array, actual type: {:?}", batch.column(0).data_type()))?;
                    
                let col_y = batch.column(1)
                    .as_any()
                    .downcast_ref::<Float64Array>()
                    .ok_or_else(|| format!("Column 1 is not Float64Array, actual type: {:?}", batch.column(1).data_type()))?;

                if col_x.len() != col_y.len() {
                    return Err(format!("Column length mismatch: X={}, Y={}", col_x.len(), col_y.len()).into());
                }

                println!("  â†’ Extracting {} points from columns", col_x.len());

                all_points.reserve(col_x.len());
                for i in 0..col_x.len() {
                    let x = if col_x.is_null(i) {
                        return Err(format!("Null value found in X column at index {}", i).into());
                    } else {
                        col_x.value(i)
                    };
                    
                    let y = if col_y.is_null(i) {
                        return Err(format!("Null value found in Y column at index {}", i).into());
                    } else {
                        col_y.value(i)
                    };
                    
                    all_points.push([x, y]);
                }
            }
        }
        Err(e) => {
            println!("  â†’ StreamReader with added header failed: {}", e);
            println!("  â†’ Trying original data with StreamReader (maybe it's already a stream)...");
            
            // å°è¯•ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆå¯èƒ½å·²ç»æ˜¯ Stream æ ¼å¼ï¼Œåªæ˜¯æ²¡æœ‰é­”æ•°ï¼‰
            let cursor_original = Cursor::new(&data);
            match StreamReader::try_new(cursor_original, None) {
                Ok(mut reader) => {
                    println!("  â†’ Using StreamReader on original data");
                    for batch_result in reader {
                        let batch = batch_result
                            .map_err(|e| format!("Failed to read batch from StreamReader: {}", e))?;
                        
                        batch_count += 1;
                        println!("ğŸ“¦ Processing batch #{}: {} columns, {} rows", 
                                 batch_count, batch.num_columns(), batch.num_rows());

                        if batch.num_columns() < 2 {
                            return Err(format!("Expected at least 2 columns, got {}", batch.num_columns()).into());
                        }

                        let col_x = batch.column(0)
                            .as_any()
                            .downcast_ref::<Float64Array>()
                            .ok_or_else(|| format!("Column 0 is not Float64Array, actual type: {:?}", batch.column(0).data_type()))?;
                            
                        let col_y = batch.column(1)
                            .as_any()
                            .downcast_ref::<Float64Array>()
                            .ok_or_else(|| format!("Column 1 is not Float64Array, actual type: {:?}", batch.column(1).data_type()))?;

                        if col_x.len() != col_y.len() {
                            return Err(format!("Column length mismatch: X={}, Y={}", col_x.len(), col_y.len()).into());
                        }

                        println!("  â†’ Extracting {} points from columns", col_x.len());

                        all_points.reserve(col_x.len());
                        for i in 0..col_x.len() {
                            let x = if col_x.is_null(i) {
                                return Err(format!("Null value found in X column at index {}", i).into());
                            } else {
                                col_x.value(i)
                            };
                            
                            let y = if col_y.is_null(i) {
                                return Err(format!("Null value found in Y column at index {}", i).into());
                            } else {
                                col_y.value(i)
                            };
                            
                            all_points.push([x, y]);
                        }
                    }
                    
                    if batch_count > 0 {
                        println!("âœ… Successfully parsed {} batches, total points: {}", batch_count, all_points.len());
                        return Ok(all_points);
                    }
                }
                Err(e2) => {
                    println!("  â†’ StreamReader on original data also failed: {}", e2);
                }
            }
            
            // å¦‚æœ StreamReader å¤±è´¥ï¼Œå°è¯• FileReaderï¼ˆArrow IPC File æ ¼å¼ï¼‰
            let cursor2 = Cursor::new(&data);
            match FileReader::try_new(cursor2, None) {
                Ok(reader) => {
                    println!("  â†’ Using FileReader (IPC File format)");
                    for batch_result in reader {
                        let batch = batch_result
                            .map_err(|e| format!("Failed to read batch from FileReader: {}", e))?;
                        
                        batch_count += 1;
                        println!("ğŸ“¦ Processing batch #{}: {} columns, {} rows", 
                                 batch_count, batch.num_columns(), batch.num_rows());

                        if batch.num_columns() < 2 {
                            return Err(format!("Expected at least 2 columns, got {}", batch.num_columns()).into());
                        }

                        let col_x = batch.column(0)
                            .as_any()
                            .downcast_ref::<Float64Array>()
                            .ok_or_else(|| format!("Column 0 is not Float64Array, actual type: {:?}", batch.column(0).data_type()))?;
                            
                        let col_y = batch.column(1)
                            .as_any()
                            .downcast_ref::<Float64Array>()
                            .ok_or_else(|| format!("Column 1 is not Float64Array, actual type: {:?}", batch.column(1).data_type()))?;

                        if col_x.len() != col_y.len() {
                            return Err(format!("Column length mismatch: X={}, Y={}", col_x.len(), col_y.len()).into());
                        }

                        println!("  â†’ Extracting {} points from columns", col_x.len());

                        all_points.reserve(col_x.len());
                        for i in 0..col_x.len() {
                            let x = if col_x.is_null(i) {
                                return Err(format!("Null value found in X column at index {}", i).into());
                            } else {
                                col_x.value(i)
                            };
                            
                            let y = if col_y.is_null(i) {
                                return Err(format!("Null value found in Y column at index {}", i).into());
                            } else {
                                col_y.value(i)
                            };
                            
                            all_points.push([x, y]);
                        }
                    }
                }
                Err(e2) => {
                    return Err(format!("Both StreamReader and FileReader failed. StreamReader: {}, FileReader: {}", e, e2).into());
                }
            }
        }
    }

    if batch_count == 0 {
        return Err("No batches found in Arrow IPC data (tried both Stream and File formats)".into());
    }

    println!("âœ… Successfully parsed {} batches, total points: {}", batch_count, all_points.len());
    Ok(all_points)
}

pub fn convert_to_arrow_ipc(outlier_result: Vec<f64>) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    // 1. å®šä¹‰ Schema (å‡è®¾è¿”å›çš„åˆ—åä¸º "result")
    let schema = Arc::new(Schema::new(vec![
        Field::new("result", DataType::Float64, false),
    ]));

    // 2. å°† Vec<f64> åŒ…è£…æˆ Arrow Array
    let array = Float64Array::from(outlier_result);

    // 3. åˆ›å»º RecordBatch
    let batch = RecordBatch::try_new(
        schema.clone(),
        vec![Arc::new(array)],
    )?;

    // 4. å°† RecordBatch åºåˆ—åŒ–ä¸º IPC å­—èŠ‚æµ (Stream æ ¼å¼)
    let mut buffer = Vec::new();
    {
        let mut writer = StreamWriter::try_new(&mut buffer, &schema)?;
        writer.write(&batch)?;
        writer.finish()?; // å¿…é¡»è°ƒç”¨ finish æ¥å†™å…¥æœ«å°¾æ ‡è®°
    }

    Ok(buffer)
}

pub fn convert_record_batch_to_arrow_ipc(batch: RecordBatch) -> Result<Vec<u8>, Box<dyn std::error::Error>> {
    // å°† RecordBatch åºåˆ—åŒ–ä¸º IPC å­—èŠ‚æµ (Stream æ ¼å¼)
    let mut buffer = Vec::new();
    {
        let mut writer = StreamWriter::try_new(&mut buffer, &batch.schema())?;
        writer.write(&batch)?;
        writer.finish()?; // å¿…é¡»è°ƒç”¨ finish æ¥å†™å…¥æœ«å°¾æ ‡è®°
    }

    Ok(buffer)
}

fn dataframe_to_recordbatch_forecast(mut df: DataFrame) -> Result<RecordBatch, Box<dyn std::error::Error>> {
    // 1. ç¡®ä¿æ•°æ®è¿ç»­
    let df = df.align_chunks();

    // 2. è·å–æ‰€æœ‰åˆ—å
    let column_names = df.get_column_names();
    let mut fields = Vec::new();
    let mut arrays: Vec<Arc<dyn ArrowArray>> = Vec::new();

    // 3. é€åˆ—è½¬æ¢
    for col_name in column_names {
        let series = df.column(col_name)?;

        // æ ¹æ®åˆ—çš„æ•°æ®ç±»å‹è¿›è¡Œè½¬æ¢ï¼ˆæ³¨æ„è¿™é‡ŒåŒ¹é…çš„æ˜¯ Polars çš„ DataTypeï¼‰
        let (field, array): (Field, Arc<dyn ArrowArray>) = match series.dtype() {
            PolarsDataType::Float64 => {
                let ca = series.f64()?;
                let values: Vec<Option<f64>> = ca.into_iter().collect();
                let arrow_array = Float64Array::from(values);
                (
                    Field::new(col_name.to_string(), DataType::Float64, true),
                    Arc::new(arrow_array),
                )
            }
            PolarsDataType::Int64 => {
                let ca = series.i64()?;
                let values: Vec<Option<i64>> = ca.into_iter().collect();
                let arrow_array = Int64Array::from(values);
                (
                    Field::new(col_name.to_string(), DataType::Int64, true),
                    Arc::new(arrow_array),
                )
            }
            _ => {
                return Err(
                    format!("ä¸æ”¯æŒçš„åˆ—ç±»å‹: {:?} for column {}", series.dtype(), col_name).into()
                );
            }
        };

        fields.push(field);
        arrays.push(array);
    }

    // 4. æ„å»º RecordBatch
    let schema = Arc::new(Schema::new(fields));
    Ok(RecordBatch::try_new(schema, arrays)?)
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    init_db().map_err(|e| {
        let err_msg = format!("init sqlite faield: {}", e);
        eprintln!("âŒ {}", err_msg);
        std::io::Error::new(std::io::ErrorKind::Other, err_msg)
    })?;
    // Unix socket è·¯å¾„
    let socket_path = "/tmp/rsod-service.sock";
    
    // å¦‚æœ socket æ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤å®ƒ
    if Path::new(socket_path).exists() {
        std::fs::remove_file(socket_path)?;
    }
    
    // åˆ›å»º UnixListener
    let uds = UnixListener::bind(socket_path)?;
    let uds_stream = UnixListenerStream::new(uds);
    
    let rsod_service = RsodServiceImpl::default();

    println!("ğŸš€ Rsod gRPC Server starting, listen Unix socket: {}", socket_path);

    Server::builder()
        .add_service(RsodServiceServer::new(rsod_service))
        .serve_with_incoming_shutdown(
            uds_stream,
            async {
                let _ = signal::ctrl_c().await;
                println!("\nExit Rsod gRPC Server...");
            },
        )
        .await?;

    Ok(())
}
